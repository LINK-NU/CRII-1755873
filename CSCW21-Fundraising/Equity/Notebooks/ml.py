#/usr/bin/python2

import timeit
import scipy
import operator
import collections
import numpy as np
import pandas as pd
from scipy import stats
# import seaborn as sns
from collections import Counter
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
# import statsmodels.api as sm
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier as MLP
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import classification_report
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.manifold.t_sne import TSNE
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from collections import OrderedDict
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_colwidth', -1)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
# sns.set_style('whitegrid')
plt.style.use('seaborn-whitegrid')

__author__ = 'HK Dambanemuya'

# Classifiers
classifiers = {"GNB": GaussianNB(), 
               "ADB": AdaBoostClassifier(), 
               "CART": DecisionTreeClassifier(), 
               "RF": RandomForestClassifier(random_state=42),
               "QDA": QuadraticDiscriminantAnalysis(),
               "LR": LogisticRegression(random_state=42)}

def evaluation_summary(X,y):
    eval_df = pd.DataFrame()
    acc_, prec_, rec_, f1_, auc_, names = [], [], [], [], [], []
    for name, clf in classifiers.items():
        names.append(name)
        start = timeit.default_timer()
        y_pred = run_cv(X,y,clf)
        acc_.append(accuracy(y, y_pred))
        prec_.append(precision(y, y_pred))
        rec_.append(recall(y, y_pred))
        f1_.append(f1(y, y_pred))
        auc_.append(auc(y, y_pred))
        stop = timeit.default_timer()
        print ("Completed {0} in {1} seconds".format(name, (stop-start)) )
    eval_df['Model'] = names
    eval_df['Accuracy'] = acc_
    eval_df['Precision'] = prec_
    eval_df['Recall'] = rec_
    eval_df['F1'] = f1_
    eval_df['AUC'] = auc_
    
    return eval_df

def run_cv(X,y,clf_class):
    """
        Takes input arrays and trained Scikit-Learn classifier and returns 
        an array of predicted classes generated by the model

        Parameters
        -------------------
        X:    Numpy array or Pandas Dataframe with shape = [n_samples, n_features]

        y:    A 1-d Numpy array or Pandas Series with shape = [n_samples]
        clf_class:  Scikit-Learn classifier object
    """
    # Construct a kfolds object
    kf = KFold(n_splits=5,shuffle=True)
    kf.get_n_splits(X)
    y_pred = y.copy()

    # Iterate through folds
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train = y[train_index]
        clf = clf_class
        clf.fit(X_train,y_train)
        y_pred[test_index] = clf.predict(X_test)
    return y_pred

def frank_summary(x, y, features, output=None):
    # create a base classifier used to evaluate a subset of attributes
    start = timeit.default_timer()
    model = classifiers.get("RF")

    # Recursive Feature Elimination (RFE)
    # create the RFE model and select 5 attributes
    rfe = RFE(model, 20)
    rfe = rfe.fit(x,y)

    ft_df = pd.DataFrame()
    ft_df['Feature'] = features
    ft_df['RFE Rank'] = rfe.ranking_
    ft_df['RFE Support'] = rfe.support_
    # ft_df['ETC Importance'] = ExtraTreesClassifier().fit(x,y).feature_importances_
    ft_df['RF Importance'] = model.fit(x,y).feature_importances_
    ft_df['x'] = range(1,len(ft_df)+1,1)
    result = ft_df.sort_values('RF Importance', ascending=False).reset_index(level=None, drop=True)
    stop = timeit.default_timer()
    print ("Feature ranking completed in {0} seconds".format((stop-start)) )
    if output=="f":
        result.to_csv("../Data/feature_ranking_results.csv", index=False)
    return result

def accuracy(y_true,y_pred): return metrics.accuracy_score(y_true, y_pred)
def precision(y_true, y_pred): return metrics.precision_score(y_true, y_pred)
def recall(y_true, y_pred): return metrics.recall_score(y_true, y_pred)
def f1(y_true, y_pred): return metrics.f1_score(y_true, y_pred)
def auc(y_true, y_pred): return metrics.roc_auc_score(y_true, y_pred)

def auc_scores(X,y):
    scores = []
    for name, clf in classifiers.items():
        scores.append(auc(y, run_cv(X,y,clf)))  
    return scores 

def plot_auc(X,y, names):
    fig, ax = plt.subplots(figsize=(13,6))

    zipped = zip(names, auc_scores(X,y))
    zipped.sort(key = lambda t: t[1])

    sns.barplot([zipp[1] for zipp in zipped], [zipp[0] for zipp in zipped], palette="RdYlGn")
    plt.xlabel("AUC")
    plt.ylabel("Classifier")
    plt.title("Classifier AUC Comparison")
    plt.show()

def plot_confusion_matrix(X,y):
    i=1
    figure = plt.figure(figsize=(12, 6))
    for name, clf in classifiers.items():
        ax = plt.subplot(2, len(classifiers)/2, i)
        ax = sns.heatmap(confusion_matrix(y,run_cv(X,y,clf)), annot=True, fmt="d", cmap="RdYlBu")
        ax.set_ylabel("Predicted Class")
        ax.set_xlabel("True Class")
        ax.set_title(name)
        i+=1  
    plt.tight_layout()
    plt.show()

def plot_roc(X,y):
    i = 1
    x = [0.0, 1.0]
    figure = plt.figure(figsize=(12, 6))
    for name, clf in classifiers.items():
        auc = roc_auc_score(y,run_cv(X,y,clf))
        fpr, tpr, _ = roc_curve(y,run_cv(X,y,clf))
        ax = plt.subplot(2, len(classifiers)/2, i)
        plt.plot(fpr, tpr, label='%.4f' % auc, linewidth=4)
        plt.plot(x, x, linestyle='dashed', color='red', linewidth=2, label='Random')
        ax.set_xlabel("FPR")
        ax.set_ylabel("TPR")
        ax.set_xlim(-0.01, 1)
        ax.set_ylim(0, 1.02)
        plt.title(name)
        plt.legend(loc="lower right")
        i += 1
    plt.tight_layout()
    plt.show()

def gini(actual, pred, weight=None):
    pdf= pd.DataFrame(scipy.vstack([actual, pred]).T, columns=['Actual', 'Predicted'],)
    pdf= pdf.sort_values('Predicted')
    if weight is None:
        pdf['Weight'] = 1.0
  
    pdf['CummulativeWeight'] = np.cumsum(pdf['Weight'])
    pdf['CummulativeWeightedActual'] = np.cumsum(pdf['Actual'] * pdf['Weight'])
    
    TotalWeight = sum(pdf['Weight'])
    Numerator = sum(pdf['CummulativeWeightedActual'] * pdf['Weight'])
    Denominator = sum(pdf['Actual']*pdf['Weight'] * TotalWeight)
    
    Gini = 1.0 - 2.0 * Numerator/Denominator
    return Gini 

def mylift(actual,pred,weight=None,n=10,xlab='Score',title=''):
    
    
    pdf = pd.DataFrame(scipy.vstack([actual,pred]).T,columns=['Actual','Predicted'],)
    pdf = pdf.sort_values('Predicted')
    if weight is None:
        pdf['Weight'] = 1.0
  
    pdf['CummulativeWeight'] = np.cumsum(pdf['Weight'])
    pdf['CummulativeWeightedActual'] = np.cumsum(pdf['Actual']*pdf['Weight'])
    TotalWeight = sum(pdf['Weight'])
    Numerator = sum(pdf['CummulativeWeightedActual']*pdf['Weight'])
    Denominator = sum(pdf['Actual']*pdf['Weight']*TotalWeight)
    Gini = 1.0 - 2.0 * Numerator/Denominator
    
    
    NormalizedGini = Gini/ gini(pdf['Actual'],pdf['Actual'])
    GiniTitle = 'Normalized Gini = '+ str(round(NormalizedGini,4))
    
    pdf['PredictedDecile'] = np.round(pdf['CummulativeWeight']*n /TotalWeight + 0.5,decimals=0)
    pdf['PredictedDecile'][pdf['PredictedDecile'] < 1.0] = 1.0
    pdf['PredictedDecile'][pdf['PredictedDecile'] > n] = n 
    
    pdf['WeightedPrediction'] = pdf['Predicted']*pdf['Weight']
    pdf['WeightedActual'] = pdf['Actual']*pdf['Weight']
    lift_df = pdf.groupby('PredictedDecile').agg({'WeightedPrediction': np.sum,'Weight':np.sum,'WeightedActual':np.sum,'PredictedDecile':np.size})
    nms = lift_df.columns.values
    nms[1] = 'Count'
    lift_df.columns = nms
    lift_df['AveragePrediction'] = lift_df['WeightedPrediction']/lift_df['Weight']
    lift_df['AverageActual'] = lift_df['WeightedActual']/lift_df['Weight']
    lift_df['AverageError'] = lift_df['AverageActual']/lift_df['AveragePrediction']
    
    d = np.arange(.1,1.1,.1)
    p = lift_df['AveragePrediction']
    a = lift_df['AverageActual'] 
    plt.plot(d,p,label='Predicted',color='blue',marker='o')
    plt.plot(d,a,label='Actual',color='red',marker='d')
    plt.legend(['Predicted','Actual'])
    plt.title(title +'\n'+GiniTitle)
    plt.xlabel(xlab)
    plt.ylabel('Default Rate')

def plot_lift(X,y):
    i = 1
    figure = plt.figure(figsize=(12, 6))
    for name, clf in classifiers.items():
        ax = plt.subplot(2, len(classifiers)/2, i)
        mylift(y, run_cv(X,y, clf), title=name)
        i += 1
    plt.tight_layout()
    plt.show()


'''
    THE CODE BELOW HAS NOT BEEN REVISED
'''

def run_cv_runtime(X,y, names):
    classifiers = [SVC, GNB, ADB, MLP, CART, RF, KNN, QDA]
    for clf, name in zip(classifiers, names):
        print (name)
        # %timeit run_cv(X,y,RF)

def run_prob_cv_runtime(X,y, names):
    classifiers = [SVC, GNB, ADB, MLP, CART, RF, KNN, QDA]
    for clf, name in zip(classifiers, names):
        print (name)
        # %timeit run_prob_cv(X,y,RF)

def run_prob_cv(X, y, clf_class, **kwargs):
    """
        Takes input arrays and trained Scikit-Learn classifier and returns 
        an array of predicted class probabilities generated by the model

        Parameters
        -------------------
        X:    Numpy array or Pandas Dataframe with shape = [n_samples, n_features]

        y:    A 1-d Numpy array or Pandas Series with shape = [n_samples]
        clf_class:  Scikit-Learn classifier object
    """
    kf = KFold(n_splits=5, shuffle=True)
    kf.get_n_splits(X)
    
    y_prob = np.zeros((len(y),2))
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train = y[train_index]
        clf = clf_class(**kwargs)
        clf.fit(X_train,y_train)
        # Predict probabilities, not classes
        y_prob[test_index] = clf.predict_proba(X_test)
    return y_prob

def compare_classifiers(Y,y):
    i = 1
    h = .02  # step size in the mesh
    figure = plt.figure(figsize=(12, 6))  
    # split into training and test part
    X_train, X_test, y_train, y_test = train_test_split(Y, y, test_size=.4, random_state=42)
    x_min, x_max = Y[:, 0].min() - .5, Y[:, 0].max() + .5
    y_min, y_max = Y[:, 1].min() - .5, Y[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    # specify color maps
    cm = plt.cm.RdYlGn_r
    cm_bright = ListedColormap(['g', 'r'])

    classifiers = [SVC(), GNB(), ADB(), MLP(), CART(), RF(), KNN(), QDA()]
    
    for name, clf, s in zip(names, classifiers, scores()):
        ax = plt.subplot(2, len(classifiers)/2, i)
        clf.fit(X_train, y_train)
        score = s
        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
        # Plot training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
                   edgecolors='k')
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   edgecolors='k', alpha=0.6)
        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.4f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1
    plt.tight_layout()
    plt.show()

def prob_summary(X,y):
    pd.set_option('display.float_format', lambda x: '%.3f' % x)

    # Use 10 estimators so predictions are all multiples of 0.1
    pred_prob = run_prob_cv(X, y, RF, n_estimators=10, random_state=42)
    # pred_prob = run_prob_cv(X, y, KNN)
    pred_churn = pred_prob[:,1]
    is_churn = y == 1

    # Number of times a predicted probability is assigned to an observation
    counts = pd.value_counts(pred_churn)

    # calculate true probabilities
    true_prob = {}
    for prob in counts.index:
        true_prob[prob] = np.mean(is_churn[pred_churn == prob])
        true_prob = pd.Series(true_prob)

    # pandas-fu
    counts = pd.concat([counts,true_prob], axis=1).reset_index()
    counts.columns = ['pred_prob', 'count', 'true_prob']
    return counts

def prob_plot(X,y):
    keys, values = [], []
    counts = prob_summary(X,y)
    data = OrderedDict(zip(counts['pred_prob'], counts['count']))
    
    for key in sorted(data.iterkeys()):
        keys.append(key)
        values.append(data[key])
    figure = plt.figure(figsize=(14, 6))
    sns.barplot(keys, values, palette="RdYlGn")
    plt.xticks(np.arange(0, 1, step=0.1))
    plt.yscale("log")
    plt.xlabel("Funding Probability")
    plt.ylabel("# Listings")
    plt.show()
